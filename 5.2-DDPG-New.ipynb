{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L96 stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96 import *\n",
    "from EnKF import *\n",
    "from utils import *\n",
    "from parameterizations import *\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initX, initY = np.load('./data/initX.npy'), np.load('./data/initY.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L96TwoLevelRL(L96TwoLevel):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def step_with_B(self, B):\n",
    "\n",
    "        k1_X = self._rhs_X_dt(self.X, B=0)\n",
    "        k2_X = self._rhs_X_dt(self.X + k1_X / 2, B=0)\n",
    "        k3_X = self._rhs_X_dt(self.X + k2_X / 2, B=0)\n",
    "        k4_X = self._rhs_X_dt(self.X + k3_X, B=0)\n",
    "\n",
    "        self.X += 1 / 6 * (k1_X + 2 * k2_X + 2 * k3_X + k4_X)\n",
    "        \n",
    "        self.X += B * self.dt\n",
    "\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.save_steps == 0:\n",
    "            Y_mean = self.Y.reshape(self.K, self.J).mean(1)\n",
    "            Y2_mean = (self.Y.reshape(self.K, self.J)**2).mean(1)\n",
    "            self._history_X.append(self.X.copy())\n",
    "            self._history_Y_mean.append(Y_mean.copy())\n",
    "            self._history_Y2_mean.append(Y2_mean.copy())\n",
    "            self._history_B.append(B.copy())\n",
    "            if not self.noYhist:\n",
    "                self._history_Y.append(self.Y.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L96Gym(gym.Env):\n",
    "    def __init__(self, lead_time, X_init, Y_init, dt=0.01, action_bounds=(-20,20)):\n",
    "        self.lead_time = lead_time\n",
    "        self.X_init, self.Y_init = X_init, Y_init\n",
    "        self.step_count = 0\n",
    "        self.dt = dt\n",
    "        self.nsteps = self.lead_time // self.dt\n",
    "        \n",
    "        self.l96_tru = L96TwoLevel(X_init=initX, Y_init=initY)\n",
    "        self.l96_tru.iterate(lead_time)\n",
    "        self.fc_target = self.l96_tru.X.copy()\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=np.array([action_bounds[0]]), \n",
    "            high=np.array([action_bounds[1]])\n",
    "        )\n",
    "        self.observation_space = gym.spaces.Box(-np.array([np.inf]), np.array([np.inf]))\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        self.l96 = L96TwoLevelRL(noYhist=True, X_init=initX, dt=self.dt)\n",
    "        state = self.l96.X\n",
    "        return state[:, None]\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.l96.step_with_B(action)\n",
    "        state = self.l96.X\n",
    "        self.step_count += 1\n",
    "        if self.step_count >= self.nsteps:\n",
    "            done = True\n",
    "            reward = -((state - self.fc_target)**2).mean()\n",
    "        else:\n",
    "            done = False\n",
    "            reward = 0\n",
    "        return state[:, None], reward, done, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Replay Buffer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBufferOld:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if state.ndim == 1:\n",
    "            self.buffer.append([state, action, reward, next_state, done])\n",
    "        else:\n",
    "            for i in range(len(state)):\n",
    "                self.buffer.append([\n",
    "                    state[i].squeeze(), action[i].squeeze(), reward, next_state[i].squeeze(), done])\n",
    "#             for s, a, ns in zip(state, action, next_state):\n",
    "#                 self.buffer.append([s, a, reward, ns, done])\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Normalize action space</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "\n",
    "    def _action(self, action):\n",
    "        low_bound   = self.action_space.low\n",
    "        upper_bound = self.action_space.high\n",
    "        \n",
    "        action = low_bound + (action + 1.0) * 0.5 * (upper_bound - low_bound)\n",
    "        action = np.clip(action, low_bound, upper_bound)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        low_bound   = self.action_space.low\n",
    "        upper_bound = self.action_space.high\n",
    "        \n",
    "        action = 2 * (action - low_bound) / (upper_bound - low_bound) - 1\n",
    "        action = np.clip(action, low_bound, upper_bound)\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Ornstein-Uhlenbeck process</h2>\n",
    "Adding time-correlated noise to the actions taken by the deterministic policy<br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process\">wiki</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise(object):\n",
    "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_space.shape[0]\n",
    "        self.low          = action_space.low\n",
    "        self.high         = action_space.high\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "    def get_action(self, action, t=0):\n",
    "        ou_state = self.evolve_state()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "        return np.clip(action + ou_state, self.low, self.high)\n",
    "    \n",
    "#https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Continuous control with deep reinforcement learning</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1509.02971\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, num_actions)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.tanh(self.linear3(x))\n",
    "        return x\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state  = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = self.forward(state)\n",
    "        return action.detach().cpu().numpy()[0, 0]\n",
    "    \n",
    "class PolicyNetworkLinear(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, num_actions)\n",
    "        aelf.linear1.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear1.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.linear1(state)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state  = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = self.forward(state)\n",
    "        return action.detach().cpu().numpy()[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DDPG Update</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg_update(batch_size, \n",
    "           gamma = 0.99,\n",
    "           min_value=-np.inf,\n",
    "           max_value=np.inf,\n",
    "           soft_tau=1e-2):\n",
    "    \n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    state      = torch.FloatTensor(state).to(device)\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    action     = torch.FloatTensor(action).to(device)\n",
    "    reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "    done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "    policy_loss = value_net(state, policy_net(state))\n",
    "    policy_loss = -policy_loss.mean()\n",
    "\n",
    "    next_action    = target_policy_net(next_state)\n",
    "    target_value   = target_value_net(next_state, next_action.detach())\n",
    "    expected_value = reward + (1.0 - done) * gamma * target_value\n",
    "    expected_value = torch.clamp(expected_value, min_value, max_value)\n",
    "\n",
    "    value = value_net(state, action)\n",
    "    value_loss = value_criterion(value, expected_value.detach())\n",
    "\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "\n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n",
    "\n",
    "    for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NormalizedActions(gym.make(\"MountainCarContinuous-v0\"))\n",
    "ou_noise = OUNoise(env.action_space)\n",
    "\n",
    "state_dim  = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "hidden_dim = 256\n",
    "\n",
    "value_net  = ValueNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "target_value_net  = ValueNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "target_policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "    \n",
    "    \n",
    "value_lr  = 1e-3\n",
    "policy_lr = 1e-4\n",
    "\n",
    "value_optimizer  = optim.Adam(value_net.parameters(),  lr=value_lr)\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "value_criterion = nn.MSELoss()\n",
    "\n",
    "replay_buffer_size = 1000000\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames  = 20000\n",
    "max_steps   = 500\n",
    "frame_idx   = 0\n",
    "rewards     = []\n",
    "batch_size  = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9752e817bb56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mddpg_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-4f6240d040c4>\u001b[0m in \u001b[0;36mddpg_update\u001b[0;34m(batch_size, gamma, min_value, max_value, soft_tau)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtarget_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_value_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             target_param.data.copy_(\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mtarget_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msoft_tau\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msoft_tau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             )\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while frame_idx < max_frames:\n",
    "    state = env.reset()\n",
    "    ou_noise.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action = policy_net.get_action(state)\n",
    "        action = ou_noise.get_action(action, step)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            ddpg_update(batch_size)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % max(1000, max_steps + 1) == 0:\n",
    "            plot(frame_idx, rewards)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    rewards.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "state=env.reset()\n",
    "env.render()\n",
    "while not done:\n",
    "    action = policy_net.get_action(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L96 Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58b35ee4e1d4cd686278a35293ff578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = NormalizedActions(L96Gym(1, initX, initY))\n",
    "ou_noise = OUNoise(env.action_space)\n",
    "\n",
    "state_dim  = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "hidden_dim = 256\n",
    "\n",
    "value_net  = ValueNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "target_value_net  = ValueNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "target_policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "    \n",
    "    \n",
    "value_lr  = 1e-3\n",
    "policy_lr = 1e-4\n",
    "\n",
    "value_optimizer  = optim.Adam(value_net.parameters(),  lr=value_lr)\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "value_criterion = nn.MSELoss()\n",
    "\n",
    "replay_buffer_size = 1000000\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames  = 20000\n",
    "max_steps   = 500\n",
    "frame_idx   = 0\n",
    "rewards     = []\n",
    "batch_size  = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.get_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 36], m2: [1 x 256] at /Users/distiller/project/conda/conda-bld/pytorch_1556653464916/work/aten/src/TH/generic/THTensorMath.cpp:961",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3523a1c46c73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mou_noise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e37a0ee64c95>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mstate\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e37a0ee64c95>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 36], m2: [1 x 256] at /Users/distiller/project/conda/conda-bld/pytorch_1556653464916/work/aten/src/TH/generic/THTensorMath.cpp:961"
     ]
    }
   ],
   "source": [
    "while frame_idx < max_frames:\n",
    "    state = env.reset()\n",
    "    ou_noise.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action = policy_net.get_action(state.squeeze()[None])\n",
    "        action = ou_noise.get_action(action, step)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            ddpg_update(batch_size)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % max(1000, max_steps + 1) == 0:\n",
    "            plot(frame_idx, rewards)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    rewards.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-9-54d08f596be5>\u001b[0m(11)\u001b[0;36mpush\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      9 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m                self.buffer.append([\n",
      "\u001b[0m\u001b[0;32m---> 11 \u001b[0;31m                    state[i].squeeze(), action[i].squeeze(), reward, next_state[i].squeeze(), done])\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m\u001b[0;31m#             for s, a, ns in zip(state, action, next_state):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m\u001b[0;31m#                 self.buffer.append([s, a, reward, ns, done])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> action\n",
      "array([0.05827501])\n",
      "ipdb> u\n",
      "> \u001b[0;32m<ipython-input-20-9de3efe19b71>\u001b[0m(11)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      9 \u001b[0;31m        \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 11 \u001b[0;31m        \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m            \u001b[0mddpg_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> action\n",
      "array([0.05827501])\n",
      "ipdb> state\n",
      "array([[ 7.13482625],\n",
      "       [ 3.13563964],\n",
      "       [ 1.50764469],\n",
      "       [ 3.4558074 ],\n",
      "       [ 3.56768157],\n",
      "       [-2.85738061],\n",
      "       [ 5.34980682],\n",
      "       [ 6.16399497],\n",
      "       [ 2.37294842],\n",
      "       [ 4.69939535],\n",
      "       [ 6.41154816],\n",
      "       [ 0.72773096],\n",
      "       [-5.56037264],\n",
      "       [ 0.73058682],\n",
      "       [ 8.6022616 ],\n",
      "       [ 4.06707857],\n",
      "       [ 0.83155202],\n",
      "       [-1.95226576],\n",
      "       [-2.01863198],\n",
      "       [ 4.06713119],\n",
      "       [ 6.00059353],\n",
      "       [ 1.79730397],\n",
      "       [-1.05087632],\n",
      "       [-1.14037896],\n",
      "       [ 5.14885867],\n",
      "       [ 4.49628239],\n",
      "       [ 3.90753351],\n",
      "       [ 2.27422305],\n",
      "       [-0.8137418 ],\n",
      "       [-2.70526047],\n",
      "       [ 6.48811814],\n",
      "       [ 5.42435322],\n",
      "       [ 3.97385693],\n",
      "       [ 1.5185496 ],\n",
      "       [-1.46986024],\n",
      "       [-2.78693026]])\n",
      "ipdb> state.shape\n",
      "(36, 1)\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.02781445,  3.02862784,  1.40063289,  3.3487956 ,  3.46066977,\n",
       "       -2.96439241,  5.24279503,  6.05698318,  2.26593662,  4.59238355,\n",
       "        6.30453636,  0.62071917, -5.66738443,  0.62357503,  8.4952498 ,\n",
       "        3.96006677,  0.72454022, -2.05927756, -2.12564378,  3.9601194 ,\n",
       "        5.89358173,  1.69029217, -1.15788812, -1.24739075,  5.04184687,\n",
       "        4.38927059,  3.80052171,  2.16721125, -0.9207536 , -2.81227227,\n",
       "        6.38110634,  5.31734142,  3.86684513,  1.4115378 , -1.57687204,\n",
       "       -2.89394206])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, action, reward, next_state, done = replay_buffer.sample(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 36), (128,))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape, reward.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
